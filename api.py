# -*- coding: utf-8 -*-
"""api.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oIwdLS6nuUNw0YPtoGfJa6-lfnn3uVNI
"""

pip install FastAPI

from typing import List, Tuple
from fastapi import FastAPI
from pydantic import BaseModel
import numpy as np
import requests

app = FastAPI()

class Phrase(BaseModel): # general definitions to the classes
    phrase: str

class PhrasePair(BaseModel):
    phrase1: str
    phrase2: str
    cosine_distance: float

class PhrasePairList(BaseModel):
    phrase_pairs: List[PhrasePair]

def calculate_cosine_distance(phrase1: np.ndarray, phrase2: np.ndarray) -> float:  # This is the cosine distance that measures the similarity between the phrases (vectors)
    return np.dot(phrase1, phrase2) / (np.linalg.norm(phrase1) * np.linalg.norm(phrase2))

def calculate_text_embedding(phrase: str) -> np.ndarray:                        # Here, a call to base model hosted at huggingface using it's api
    url = "https://api-inference.huggingface.co/models/BAAI/bge-base-en-v1.5"   # the data is sent to the api and it embeds it
    headers = {"Authorization":  "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"}       # this function is part of the logic, and its designed to ensure text embedment using huggingface api
    data = {"inputs": phrase}
    response = requests.post(url, headers=headers, json=data)
    return np.array(response.json()[0]["vector"])

def get_top_2_closest_phrases(phrases: List[str]) -> Tuple[str, str]:           # this function basically calls upon the previous one and returns the closest 2 according to the cosine
    phrase_embeddings = [calculate_text_embedding(phrase) for phrase in phrases] # distance. What to note here is, the cosine distance is found via words embedment so numerical values
    phrase_pairs = []                                                            # were assigned to words to create vectors
    for i in range(len(phrases)):
        for j in range(i + 1, len(phrases)):
            cosine_distance = calculate_cosine_distance(phrase_embeddings[i], phrase_embeddings[j])
            phrase_pairs.append(PhrasePair(phrase1=phrases[i], phrase2=phrases[j], cosine_distance=cosine_distance))
    phrase_pairs.sort(key=lambda x: x.cosine_distance)
    return phrase_pairs[0].phrase1, phrase_pairs[0].phrase2

@app.post("/top2", response_model=PhrasePairList) # This is the final block of the logic
def top2(phrases: List[Phrase]) -> PhrasePairList: # sends a POST request with top2 and waits to get back from the
    phrase_list = [phrase.phrase for phrase in phrases] # hugging face api with the closest 2, and it returns these closest 2
    phrase1, phrase2 = get_top_2_closest_phrases(phrase_list)
    return PhrasePairList(phrase_pairs=[PhrasePair(phrase1=phrase1, phrase2=phrase2, cosine_distance=0)])

!pip install fastapi uvicorn colab_host

uvicorn app:app --reload  # to give me a local host url to pass it to the demo file to finally get the output